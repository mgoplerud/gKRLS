% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/main_function.R
\encoding{UTF-8}
\name{gKRLS}
\alias{gKRLS}
\title{Generalized Kernel Regularized Least Squares}
\usage{
gKRLS(
  truncate.eigen.tol = sqrt(.Machine$double.eps),
  demean_kernel = FALSE,
  sketch_method = "nystrom",
  standardize = "Mahalanobis",
  sketch_multiplier = 5,
  sketch_size_raw = NULL,
  sketch_prob = NULL,
  rescale_penalty = TRUE,
  remove_instability = TRUE
)
}
\arguments{
\item{truncate.eigen.tol}{Remove columns of the penalty, i.e. \code{S^T K S},
whose eigenvalue is below \code{truncate.eigen.tol}. This ensures a
numerically positive-definite penalty. These columns are also removed from
the sketched kernel. Default is `sqrt(.Machine$double.eps)`. Setting to 0
retains all numerically non-negative eigenvalues. This helps with the
numerical stability of the algorithm. Removal can be disabled using
\code{remove_instability}.}

\item{demean_kernel}{A logical variable \code{TRUE} indicates whether columns
of the (sketched) kernel should be demeaned before estimation. The default
is \code{FALSE}.}

\item{sketch_method}{A string that specifies which kernel sketch method should
be used. Options include \code{"nystrom"} (Nyström), \code{"gaussian"},
\code{"bernoulli"}, or \code{"none"} (no sketching). Default is
\code{"nystrom"}. See Drineas et al. (2005) and Yang et al. (2017) for details.}

\item{standardize}{A string that specifies how the data is standardized
before distance between observations is calculated. The default is
\code{"Mahalanobis"}. Other options are \code{"scaled"} (ensure all
non-constant columns are mean zero and variance one) or \code{"none"} (no
standardization).}

\item{sketch_multiplier}{By default, sketching size increases with \code{c *
ceiling(nrow(X)^(1/3))} where \code{c} is the "multiplier". Default of 5;
if results seems unstable, Chang and Goplerud (2022) find that 15 works
well.}

\item{sketch_size_raw}{Set the exact sketching size (independent of N).
Exactly one of this or sketch_multiplier must be \code{NULL}.}

\item{sketch_prob}{For bernoulli sketching, what is probability of "1"? See
Yang et al. (2017) for details.}

\item{rescale_penalty}{Rescale penalty for numerical stability; see
documentation for \code{mgcv::smooth.spec} on the meaning of this term.
Default of \code{TRUE}.}

\item{remove_instability}{A logical variable that indicates whether numerical
zeros (set via \code{truncate.eigen.tol}) should be removed when building
the penalty matrix. The default is \code{TRUE}.}
}
\description{
This page documents how to estimate \code{gKRLS} using \code{mgcv}'s
functions, e.g. \code{bam} or \code{gam}. \code{gKRLS} can be specified as
shown in the accompanying examples. Post-estimation functions to calculate
marginal effects are documented elsewhere, e.g. \link{calculate_effects}.
}
\details{
The \code{gKRLS} function should not be called directly and is a control
argument to the smoother in \code{mgcv}, i.e. \code{s(..., bs = "gKRLS", xt =
gKRLS(...)}. Its arguments are described below. Multiple kernels can be
included alongside other smooth arguments specified via \code{s(...)}.

\bold{Note:} Variables must be separated with commas inside of \code{s(...)}.
}
\examples{
n <- 100
x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- rnorm(n)
state <- sample(letters[1:5], n, replace = TRUE)
y <- 0.3 * x1 + 0.4 * x2 + 0.5 * x3 + rnorm(n)
data <- data.frame(y, x1, x2, x3, state)
data$state <- factor(data$state)
# A gKRLS model without fixed effects
gkrls_est <- mgcv::gam(y ~ s(x1, x2, x3, bs = "gKRLS"), data = data)
summary(gkrls_est)
# A gKRLS model with fixed effects
gkrls_fx <- mgcv::gam(y ~ state + s(x1, x2, x3, bs = "gKRLS"), data = data)
# Change default standardization to Mahalanobis, sketch method to Gaussian,
# and alter sketching multiplier
gkrls_mah <- mgcv::gam(y ~ s(x1, x2, x3,
  bs = "gKRLS",
  xt = gKRLS(
    standardize = "Mahalanobis",
    sketch_method = "gaussian",
    sketch_multiplier = 2
  )
),
data = data
)

# calculate marginal effect
calculate_effects(gkrls_est, variables = "x1", continuous_type = "derivative")
}
\references{
Chang, Qing and Max Goplerud. 2022. "Generalized Kernel Regularized Least
Squares".

Drineas, Petros and Mahoney, Michael W and Nello Cristianini. 2005. "On the
Nyström Method for Approximating a Gram Matrix For Improved Kernel-Based
Learning". \emph{Journal of Machine Learning Research} 6(12):2153-2175.

Yang, Yun and Pilanci, Mert and Martin J. Wainwright. 2017. "Randomized
Sketches for Kernels: Fast and Optimal Nonparametric Regression".
\emph{Annals of Statistics} 45(3):991-1023.
}
